{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer \n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import naive_bayes\n",
    "import string \n",
    "from sklearn.svm import LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/Users/rouzbehrahimi/Desktop/IE-BD/NLP/First_pro/fake_or_real_news_training.csv')#index_col='ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('/Users/rouzbehrahimi/Desktop/IE-BD/NLP/First_pro/fake_or_real_news_test.csv')#index_col='ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check the two data set shapes and make them unifrom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \n",
       "0  September New Homes Sales Rise Back To 1992 Le...  \n",
       "1  But when Congress debated and passed the Patie...  \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...  \n",
       "3  Police searching for the second of two escaped...  \n",
       "4  No matter who wins California's 475 delegates ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['X2'].isna()==False ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['X1'].isna()==False ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if we have duplicated IDs\n",
    "\n",
    "and as we can see there is no duplicated IDs in neither of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, title, text, label, X1, X2]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, title, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As only 35 rows out of 4000 have values in X1 and X2 columns we decided to drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['X1','X2'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As for the feature engineering process we would stack the train and test set, we should make their structure similar therefore we add the label column to the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text label  \n",
       "0  September New Homes Sales Rise Back To 1992 Le...  None  \n",
       "1  But when Congress debated and passed the Patie...  None  \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...  None  \n",
       "3  Police searching for the second of two escaped...  None  \n",
       "4  No matter who wins California's 475 delegates ...  None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and test shape before deleting some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2321, 4), (3999, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape,train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As described before we delete the rows with wrong structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_delet=train[(train['label']!='FAKE') & (train['label']!='REAL' )].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(rows_to_delet,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and test shape after deleting some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2321, 4), (3966, 4))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape,train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As this proplem is a classification problem it is quite important to check and see if the dataset is balanced or not\n",
    "The countplot below shows that the dataset is balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a205fcb00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax=plt.subplots(figsize=(8,6))\n",
    "sns.countplot('label',data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "As we will try differnet methods and variables for finding the best method here we make concat both text and title column in and name them 'mix' which would be used later in the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mix']=train['title']+train['text']\n",
    "test['mix']=test['title']+test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>You Can Smell Hillary’s FearDaniel Greenfield,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathyU.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                                 mix  \n",
       "0  You Can Smell Hillary’s FearDaniel Greenfield,...  \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...  \n",
       "2  Kerry to go to Paris in gesture of sympathyU.S...  \n",
       "3  Bernie supporters on Twitter erupt in anger ag...  \n",
       "4  The Battle of New York: Why This Primary Matte...  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>None</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>None</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>None</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  September New Homes Sales Rise Back To 1992 Le...  None   \n",
       "1  But when Congress debated and passed the Patie...  None   \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...  None   \n",
       "3  Police searching for the second of two escaped...  None   \n",
       "4  No matter who wins California's 475 delegates ...  None   \n",
       "\n",
       "                                                 mix  \n",
       "0  September New Homes Sales Rise——-Back To 1992 ...  \n",
       "1  Why The Obamacare Doomsday Cult Can't Admit It...  \n",
       "2  Sanders, Cruz resist pressure after NY losses,...  \n",
       "3  Surviving escaped prisoner likely fatigued and...  \n",
       "4  Clinton and Sanders neck and neck in Californi...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing train and test seprately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train_lemmatization\n",
    "This piece of code lemmatize the mix column using the wordnet dictionary to find the pos_tag of words first and then does the lemmatization. It also remove stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "train[\"mix\"] = [entry.lower() for entry in train[\"mix\"]]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "train[\"mix\"] = [word_tokenize(entry) for entry in train[\"mix\"]]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map[\"J\"] = wn.ADJ\n",
    "tag_map[\"V\"] = wn.VERB\n",
    "tag_map[\"R\"] = wn.ADV\n",
    "for index, entry in enumerate(train[\"mix\"]):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    train.loc[index, \"mix_final\"] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "      <th>mix_final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, feardaniel, g...</td>\n",
       "      <td>['smell', 'hillary', 'feardaniel', 'greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>['watch', 'exact', 'moment', 'paul', 'ryan', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>['kerry', 'go', 'paris', 'gesture', 'secretary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>['bernie', 'supporter', 'twitter', 'erupt', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[the, battle, of, new, york, :, why, this, pri...</td>\n",
       "      <td>['battle', 'new', 'york', 'primary', 'mattersi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \\\n",
       "ID                                                               \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "875    It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                                     mix  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, feardaniel, g...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, :, why, this, pri...   \n",
       "\n",
       "                                               mix_final  \n",
       "ID                                                        \n",
       "8476   ['smell', 'hillary', 'feardaniel', 'greenfield...  \n",
       "10294  ['watch', 'exact', 'moment', 'paul', 'ryan', '...  \n",
       "3608   ['kerry', 'go', 'paris', 'gesture', 'secretary...  \n",
       "10142  ['bernie', 'supporter', 'twitter', 'erupt', 'a...  \n",
       "875    ['battle', 'new', 'york', 'primary', 'mattersi...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test_lemmatization\n",
    "This piece of code lemmatize the mix column using the wordnet dictionary to find the pos_tag of words first and then does the lemmatization. It also remove stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "test[\"mix\"] = [entry.lower() for entry in test[\"mix\"]]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "test[\"mix\"] = [word_tokenize(entry) for entry in test[\"mix\"]]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map[\"J\"] = wn.ADJ\n",
    "tag_map[\"V\"] = wn.VERB\n",
    "tag_map[\"R\"] = wn.ADV\n",
    "for index, entry in enumerate(test[\"mix\"]):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    test.loc[index, \"mix_final\"] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.set_index('ID',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking train and test dataset for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "      <th>mix_final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, feardaniel, g...</td>\n",
       "      <td>['smell', 'hillary', 'feardaniel', 'greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>['watch', 'exact', 'moment', 'paul', 'ryan', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>['kerry', 'go', 'paris', 'gesture', 'secretary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>['bernie', 'supporter', 'twitter', 'erupt', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[the, battle, of, new, york, :, why, this, pri...</td>\n",
       "      <td>['battle', 'new', 'york', 'primary', 'mattersi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \\\n",
       "ID                                                               \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "875    It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                                     mix  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, feardaniel, g...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, :, why, this, pri...   \n",
       "\n",
       "                                               mix_final  \n",
       "ID                                                        \n",
       "8476   ['smell', 'hillary', 'feardaniel', 'greenfield...  \n",
       "10294  ['watch', 'exact', 'moment', 'paul', 'ryan', '...  \n",
       "3608   ['kerry', 'go', 'paris', 'gesture', 'secretary...  \n",
       "10142  ['bernie', 'supporter', 'twitter', 'erupt', 'a...  \n",
       "875    ['battle', 'new', 'york', 'primary', 'mattersi...  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the result into a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv ('/Users/rouzbehrahimi/desktop/IE-BD/NLP/new_df_2.csv', index = 'ID', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/rouzbehrahimi/desktop/IE-BD/NLP/new_df.csv',index_col = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "      <th>mix_final</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>['you', 'can', 'smell', 'hillary', '’', 's', '...</td>\n",
       "      <td>['smell', 'hillary', 'feardaniel', 'greenfield...</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>['watch', 'the', 'exact', 'moment', 'paul', 'r...</td>\n",
       "      <td>['watch', 'exact', 'moment', 'paul', 'ryan', '...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>['kerry', 'to', 'go', 'to', 'paris', 'in', 'ge...</td>\n",
       "      <td>['kerry', 'go', 'paris', 'gesture', 'secretary...</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>['bernie', 'supporters', 'on', 'twitter', 'eru...</td>\n",
       "      <td>['bernie', 'supporter', 'twitter', 'erupt', 'a...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>['the', 'battle', 'of', 'new', 'york', ':', 'w...</td>\n",
       "      <td>['battle', 'new', 'york', 'primary', 'mattersi...</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                                mix  \\\n",
       "ID                                                               \n",
       "8476   FAKE  ['you', 'can', 'smell', 'hillary', '’', 's', '...   \n",
       "10294  FAKE  ['watch', 'the', 'exact', 'moment', 'paul', 'r...   \n",
       "3608   REAL  ['kerry', 'to', 'go', 'to', 'paris', 'in', 'ge...   \n",
       "10142  FAKE  ['bernie', 'supporters', 'on', 'twitter', 'eru...   \n",
       "875    REAL  ['the', 'battle', 'of', 'new', 'york', ':', 'w...   \n",
       "\n",
       "                                               mix_final  \\\n",
       "ID                                                         \n",
       "8476   ['smell', 'hillary', 'feardaniel', 'greenfield...   \n",
       "10294  ['watch', 'exact', 'moment', 'paul', 'ryan', '...   \n",
       "3608   ['kerry', 'go', 'paris', 'gesture', 'secretary...   \n",
       "10142  ['bernie', 'supporter', 'twitter', 'erupt', 'a...   \n",
       "875    ['battle', 'new', 'york', 'primary', 'mattersi...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...   \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...   \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...   \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...   \n",
       "875    It's primary day in New York and front-runners...   \n",
       "\n",
       "                                                   title  \n",
       "ID                                                        \n",
       "8476                        You Can Smell Hillary’s Fear  \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...  \n",
       "3608         Kerry to go to Paris in gesture of sympathy  \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...  \n",
       "875     The Battle of New York: Why This Primary Matters  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6320, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_tfidf=tfidf.fit_transform(df['mix_final']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df=pd.DataFrame(mix_tfidf,columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6320, 55477)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaahhh</th>\n",
       "      <th>aab</th>\n",
       "      <th>aadhar</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aae</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaj</th>\n",
       "      <th>aakar</th>\n",
       "      <th>...</th>\n",
       "      <th>عن</th>\n",
       "      <th>لم</th>\n",
       "      <th>ما</th>\n",
       "      <th>محاولات</th>\n",
       "      <th>مطالعاتی</th>\n",
       "      <th>من</th>\n",
       "      <th>مورد</th>\n",
       "      <th>هذا</th>\n",
       "      <th>والمرضى</th>\n",
       "      <th>کدآماییposted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaahhh  aab  aadhar  aadmi  aae  aah  aaj  aakar      ...        \\\n",
       "0  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "1  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "2  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "3  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "4  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "\n",
       "    عن   لم   ما  محاولات  مطالعاتی   من  مورد  هذا  والمرضى  کدآماییposted  \n",
       "0  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "1  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "2  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "3  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "4  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "\n",
       "[5 rows x 55477 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=mix_df.iloc[:3966,]\n",
    "X_test=mix_df.iloc[3966:,]\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88964475, 0.89409985, 0.8985617 ])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "cross_validation = cross_val_score(lr, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8982401942022931"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=naive_bayes.MultinomialNB()\n",
    "NB.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(NB, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78684807, 0.79727685, 0.78576836])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Svm  with mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=mix_df.iloc[:3966,]\n",
    "X_test=mix_df.iloc[3966:,]\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaahhh</th>\n",
       "      <th>aab</th>\n",
       "      <th>aadhar</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aae</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaj</th>\n",
       "      <th>aakar</th>\n",
       "      <th>...</th>\n",
       "      <th>عن</th>\n",
       "      <th>لم</th>\n",
       "      <th>ما</th>\n",
       "      <th>محاولات</th>\n",
       "      <th>مطالعاتی</th>\n",
       "      <th>من</th>\n",
       "      <th>مورد</th>\n",
       "      <th>هذا</th>\n",
       "      <th>والمرضى</th>\n",
       "      <th>کدآماییposted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaahhh  aab  aadhar  aadmi  aae  aah  aaj  aakar      ...        \\\n",
       "0  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "1  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "2  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "3  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "4  0.0  0.0     0.0  0.0     0.0    0.0  0.0  0.0  0.0    0.0      ...         \n",
       "\n",
       "    عن   لم   ما  محاولات  مطالعاتی   من  مورد  هذا  والمرضى  کدآماییposted  \n",
       "0  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "1  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "2  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "3  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "4  0.0  0.0  0.0      0.0       0.0  0.0   0.0  0.0      0.0            0.0  \n",
       "\n",
       "[5 rows x 55475 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92441421, 0.92133132, 0.91370174])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "#SVM.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "# predict the labels on validation dataset\n",
    "#predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "#print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed: 44.5min remaining: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 46.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'C': [0.1, 1, 10]}, pre_dispatch='2*n_jobs', refit=True,\n",
       "       return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"C\": [0.1,1,10]\n",
    "    \n",
    "}\n",
    "SVM = svm.SVC(kernel='linear', degree=3, gamma='auto')\n",
    "grid_searcher = GridSearchCV(SVM, parameters, n_jobs=-1, verbose=2,cv=3)\n",
    "grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9198184568835098"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searcher.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that our initial guess for choosing C=1 was not a bad one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving more importance to title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to give more importance to title column by including it twic in the mix column and we call the mixed column weighted_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w=train\n",
    "test_w=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w['weighted_mix']=train_w['title']+train_w['mix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w['weighted_mix']=test_w['title']+test_w['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "train_w[\"weighted_mix\"] = [entry.lower() for entry in train_w[\"weighted_mix\"]]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "train_w[\"weighted_mix\"] = [word_tokenize(entry) for entry in train_w[\"weighted_mix\"]]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map[\"J\"] = wn.ADJ\n",
    "tag_map[\"V\"] = wn.VERB\n",
    "tag_map[\"R\"] = wn.ADV\n",
    "for index, entry in enumerate(train_w[\"weighted_mix\"]):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    train_w.loc[index, \"weighted_mix_final\"] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "test_w[\"weighted_mix\"] = [entry.lower() for entry in test_w[\"weighted_mix\"]]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "test_w[\"weighted_mix\"] = [word_tokenize(entry) for entry in test_w[\"weighted_mix\"]]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map[\"J\"] = wn.ADJ\n",
    "tag_map[\"V\"] = wn.VERB\n",
    "tag_map[\"R\"] = wn.ADV\n",
    "for index, entry in enumerate(test_w[\"weighted_mix\"]):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    test_w.loc[index, \"weighted_mix_final\"] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w.set_index('ID',inplace=True)\n",
    "train_w.set_index('ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>mix</th>\n",
       "      <th>weighted_mix</th>\n",
       "      <th>weighted_mix_final</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>None</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>[september, new, homes, sales, rise——-back, to...</td>\n",
       "      <td>['september', 'new', 'home', 'sale', 'level', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>None</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>[why, the, obamacare, doomsday, cult, ca, n't,...</td>\n",
       "      <td>['obamacare', 'doomsday', 'cult', 'ca', 'admit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>None</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>[sanders, ,, cruz, resist, pressure, after, ny...</td>\n",
       "      <td>['sander', 'cruz', 'resist', 'pressure', 'ny',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>None</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>[surviving, escaped, prisoner, likely, fatigue...</td>\n",
       "      <td>['survive', 'escape', 'prisoner', 'likely', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>[clinton, and, sanders, neck, and, neck, in, c...</td>\n",
       "      <td>['clinton', 'sander', 'neck', 'neck', 'califor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text label  \\\n",
       "ID                                                               \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...  None   \n",
       "2439   But when Congress debated and passed the Patie...  None   \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...  None   \n",
       "4128   Police searching for the second of two escaped...  None   \n",
       "662    No matter who wins California's 475 delegates ...  None   \n",
       "\n",
       "                                                     mix  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                            weighted_mix  \\\n",
       "ID                                                         \n",
       "10498  [september, new, homes, sales, rise——-back, to...   \n",
       "2439   [why, the, obamacare, doomsday, cult, ca, n't,...   \n",
       "864    [sanders, ,, cruz, resist, pressure, after, ny...   \n",
       "4128   [surviving, escaped, prisoner, likely, fatigue...   \n",
       "662    [clinton, and, sanders, neck, and, neck, in, c...   \n",
       "\n",
       "                                      weighted_mix_final  \n",
       "ID                                                        \n",
       "10498  ['september', 'new', 'home', 'sale', 'level', ...  \n",
       "2439   ['obamacare', 'doomsday', 'cult', 'ca', 'admit...  \n",
       "864    ['sander', 'cruz', 'resist', 'pressure', 'ny',...  \n",
       "4128   ['survive', 'escape', 'prisoner', 'likely', 'f...  \n",
       "662    ['clinton', 'sander', 'neck', 'neck', 'califor...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the weighted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w=train_w.append(test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6287, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range = (1,1))\n",
    "mix_w_tfidf=tfidf.fit_transform(df_w[\"weighted_mix_final\"]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df_w=pd.DataFrame(mix_w_tfidf,columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6287, 48528)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_df_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with weighted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=mix_df_w.iloc[:3966,]\n",
    "X_test=mix_df_w.iloc[3966:,]\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88284203, 0.89107413, 0.89326268])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "cross_validation = cross_val_score(lr, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78382464, 0.79425113, 0.78879637])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB=naive_bayes.MultinomialNB()\n",
    "NB.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(NB, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with weighted dataset\n",
    "It seems that even after giving more weights to title the result did not change. So the initial assumtion that title would be more important than the text did not hold in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92139078, 0.92284418, 0.91597275])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "#SVM.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "# predict the labels on validation dataset\n",
    "#predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "#print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92365835, 0.92511346, 0.91597275])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM=LinearSVC()\n",
    "SVM.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chi_square test for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aiming to maximize the SVM performance we would narrow down the numebr fo feature through using chi2 test and SelectKbest fuinction. we will do the below for loop to find the best number of the words which seems to be 14000 according to results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 10000 best words cross validation score is 0.9251119094749671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 12000 best words cross validation score is 0.9230941911172231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 13000 best words cross validation score is 0.9246076230402722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 14000 best words cross validation score is 0.9253644344382908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 15000 best words cross validation score is 0.9246083859551334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 16000 best words cross validation score is 0.9248601474265046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 17000 best words cross validation score is 0.9246076227517265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 18000 best words cross validation score is 0.9256160050366739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 19000 best words cross validation score is 0.9233469060878977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 20000 best words cross validation score is 0.923599049593791\n"
     ]
    }
   ],
   "source": [
    "for i in [10000,12000,13000,14000,15000,16000,17000,18000,19000,20000]:\n",
    "    X_train=mix_df_w.iloc[:3966,]\n",
    "    X_test=mix_df_w.iloc[3966:,]\n",
    "    best_k=i\n",
    "    selector = SelectKBest(chi2, k=best_k)\n",
    "    fit=selector.fit(X_train, y_train)\n",
    "#best_words = selector.get_support().nonzero()\n",
    "\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['word','Score']\n",
    "    best_words=featureScores.nlargest(best_k,'Score').word.values\n",
    "    X_train=X_train.loc[:,best_words]\n",
    "    X_test =X_test.loc[:,best_words]\n",
    "    SVM=LinearSVC()\n",
    "    SVM.fit(X_train,y_train)\n",
    "    cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n",
    "    \n",
    "    print(f'with {i} best words cross validation score is {cross_validation.mean()}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it can be seen above we are getting the best result from K=18000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    }
   ],
   "source": [
    "X_train=mix_df_w.iloc[:3966,]\n",
    "X_test=mix_df_w.iloc[3966:,]\n",
    "best_k=18000\n",
    "selector = SelectKBest(chi2, k=best_k)\n",
    "fit=selector.fit(X_train, y_train)\n",
    "#best_words = selector.get_support().nonzero()\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['word','Score']\n",
    "best_words=featureScores.nlargest(best_k,'Score').word.values\n",
    "X_train=X_train.loc[:,best_words]\n",
    "X_test =X_test.loc[:,best_words]\n",
    "SVM_final=LinearSVC()\n",
    "SVM_final.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=SVM_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('/Users/rouzbehrahimi/Desktop/IE-BD/NLP/First_pro/fake_or_real_news_test.csv')#index_col='ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2321, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = submission.to_csv ('NLP_submission.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We exported the submission file here, but kept experiencing new things that unfortunately did not yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams, chi square and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range = (1,3))\n",
    "mix_w_tfidf=tfidf.fit_transform(df_w[\"weighted_mix_final\"]).todense()\n",
    "mix_df_w=pd.DataFrame(mix_w_tfidf,columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6287, 2499340)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_df_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=mix_df_w.iloc[:3966,]\n",
    "X_test=mix_df_w.iloc[3966:,]\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:167: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    }
   ],
   "source": [
    "best_k=18000\n",
    "selector = SelectKBest(chi2, k=best_k)\n",
    "fit=selector.fit(X_train, y_train)\n",
    "#best_words = selector.get_support().nonzero()\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['word','Score']\n",
    "best_words=featureScores.nlargest(best_k,'Score').word.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:,best_words]\n",
    "X_test =X_test.loc[:,best_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM=LinearSVC()\n",
    "SVM.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(SVM, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91232048, 0.91754917, 0.91294474])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
